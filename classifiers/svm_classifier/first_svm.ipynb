{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:18:43.301002Z",
     "start_time": "2019-08-21T19:18:43.298037Z"
    },
    "colab_type": "text",
    "id": "ag-yRP2rO5wn"
   },
   "source": [
    "# Baseline tarea 1\n",
    "## Primera Iteración\n",
    "-----------------------------\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Importar librerías y utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:52.624502Z",
     "start_time": "2019-08-21T19:45:48.613907Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "fWLZ8wA0O5ww"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, cohen_kappa_score, classification_report, accuracy_score, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.sentiment.util import mark_negation\n",
    "\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OggxcxoCO5w-"
   },
   "source": [
    "## Datos\n",
    "\n",
    "### Obtener los datasets desde el github del curso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.819485Z",
     "start_time": "2019-08-21T19:45:52.626520Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "7d5pB9dsO5xB"
   },
   "outputs": [],
   "source": [
    "train = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/anger-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/fear-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/joy-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/train/sadness-train.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'])\n",
    "}\n",
    "\n",
    "target = {\n",
    "    'anger': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/anger-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'fear': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/fear-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'joy': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/joy-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE']),\n",
    "    'sadness': pd.read_csv('https://raw.githubusercontent.com/dccuchile/CC6205/master/assignments/assignment_1/data/target/sadness-target.txt', sep='\\t', names=['id', 'tweet', 'class', 'sentiment_intensity'], na_values=['NONE'])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UwaBrBQwO5xM"
   },
   "source": [
    "### Analizar los datos \n",
    "\n",
    "Imprimir la cantidad de tweets de cada dataset, según su intensidad de sentimiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.879257Z",
     "start_time": "2019-08-21T19:45:55.826364Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2119,
     "status": "ok",
     "timestamp": 1569271015748,
     "user": {
      "displayName": "F.I. S.B.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDkeCvx5V5R3vmarSYx3LSktvwQxqeHN1TprOEk2Q=s64",
      "userId": "16803044934265976430"
     },
     "user_tz": 180
    },
    "id": "69D37cVuO5xO",
    "outputId": "eb360f98-c428-4397-b4b4-ef62c4ddaf1c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anger \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 163    163    163\n",
      "low                  161    161    161\n",
      "medium               617    617    617\n",
      "fear \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 270    270    270\n",
      "low                  288    288    288\n",
      "medium               699    699    699\n",
      "joy \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 195    195    195\n",
      "low                  219    219    219\n",
      "medium               488    488    488\n",
      "sadness \n",
      "                       id  tweet  class\n",
      "sentiment_intensity                   \n",
      "high                 197    197    197\n",
      "low                  210    210    210\n",
      "medium               453    453    453\n"
     ]
    }
   ],
   "source": [
    "def get_group_dist(group_name, train):\n",
    "    print(group_name, \"\\n\",\n",
    "          train[group_name].groupby('sentiment_intensity').count())\n",
    "\n",
    "\n",
    "for key in train:\n",
    "    get_group_dist(key, train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51Q3K6cmO5xc"
   },
   "source": [
    "## Clasificar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.891188Z",
     "start_time": "2019-08-21T19:45:55.882211Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "M_nNREymO5xe"
   },
   "outputs": [],
   "source": [
    "# Metrica de evaluación. No tocar...\n",
    "def auc(test_set, predicted_set):\n",
    "    high_predicted = np.array([prediction[2] for prediction in predicted_set])\n",
    "    medium_predicted = np.array(\n",
    "        [prediction[1] for prediction in predicted_set])\n",
    "    low_predicted = np.array([prediction[0] for prediction in predicted_set])\n",
    "\n",
    "    high_test = np.where(test_set == 'high', 1.0, 0.0)\n",
    "    medium_test = np.where(test_set == 'medium', 1.0, 0.0)\n",
    "    low_test = np.where(test_set == 'low', 1.0, 0.0)\n",
    "\n",
    "    auc_high = roc_auc_score(high_test, high_predicted)\n",
    "    auc_med = roc_auc_score(medium_test, medium_predicted)\n",
    "    auc_low = roc_auc_score(low_test, low_predicted)\n",
    "\n",
    "    auc_w = (low_test.sum() * auc_low + medium_test.sum() * auc_med +\n",
    "             high_test.sum() * auc_high) / (\n",
    "                 low_test.sum() + medium_test.sum() + high_test.sum())\n",
    "    return auc_w    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "89OXoatnO5xm"
   },
   "source": [
    "### Dividir el dataset en entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:45:55.901161Z",
     "start_time": "2019-08-21T19:45:55.893181Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "tRNnXvmAO5xp"
   },
   "outputs": [],
   "source": [
    "def split_dataset(dataset):\n",
    "    # Dividir el dataset en train set y test set\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        dataset.tweet,\n",
    "        dataset.sentiment_intensity,\n",
    "        shuffle=True,\n",
    "        test_size=0.33)\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JNUvG2ZhO5xx"
   },
   "source": [
    "### Definir el clasificador\n",
    "\n",
    "Consejo para el vectorizador: investigar los modulos de `nltk`, en particular, `TweetTokenizer`, `mark_negation`. También, el parámetro ngram_range para clasificadores no bayesianos.\n",
    "\n",
    "Consejo para el clasificador: investigar otros clasificadores mas efectivos que naive bayes. Ojo q naive bayes no debería usarse con n-gramas, ya que rompe el supuesto de independencia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2091,
     "status": "ok",
     "timestamp": 1569271015755,
     "user": {
      "displayName": "F.I. S.B.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDkeCvx5V5R3vmarSYx3LSktvwQxqeHN1TprOEk2Q=s64",
      "userId": "16803044934265976430"
     },
     "user_tz": 180
    },
    "id": "1ulTa4Xj98E1",
    "outputId": "0a7fd3e9-e0da-4f92-8c91-efb1aba1bbbe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[':', 'This', 'is', 'waaayyy', 'tooo', 'much', 'for', 'you', '!', '!', '!']"
      ]
     },
     "execution_count": 363,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Experimentación\n",
    "#  strip_handles: elimina el usuario que emite el tweet\n",
    "#  reduce_len: estandariza largos, si cualquier elemento len(elem)> 3, se dejan 3 de elem\n",
    "# Ejemplo:\n",
    "tknzr = TweetTokenizer(strip_handles=False, reduce_len=False)\n",
    "s1 = '@remy: This is waaaaayyyy toooooo much for you!!!!!!'\n",
    "tknzr.tokenize(s1)\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=False)\n",
    "s1 = '@remy: This is waaaaayyyy toooooo much for you!!!!!!'\n",
    "tknzr.tokenize(s1)\n",
    "\n",
    "tknzr = TweetTokenizer(strip_handles=True, reduce_len=True) # Más útil\n",
    "s1 = '@remy: This is waaaaayyyy toooooo much for you!!!!!!'\n",
    "tknzr.tokenize(s1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:50:52.114345Z",
     "start_time": "2019-08-21T19:50:52.110384Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "YoKB6Yg2O5x0"
   },
   "outputs": [],
   "source": [
    "# Definimos el pipeline con el vectorizador y el clasificador.\n",
    "def get_classifier(c_value=1.0):\n",
    "    # Inicializamos el Vectorizador\n",
    "    vectorizor = CountVectorizer(tokenizer='callable', preprocessor='callable', ngram_range = (1,4), stop_words='english')\n",
    "    vectorizor.tokenizer = TweetTokenizer(strip_handles=True, reduce_len=True).tokenize\n",
    "    vectorizor.preprocessor = mark_negation\n",
    "    \n",
    "    # Inicializamos el Clasificador: Aquí cambiamos el clasificador por algo especializado en multiples clases\n",
    "    oneforall = OneVsRestClassifier(SVC(kernel='linear', C= c_value, probability=True, gamma='scale'))\n",
    "    \n",
    "    # Establecer el pipeline.\n",
    "    text_clf = Pipeline([('vect', vectorizor), ('clf', oneforall)])\n",
    "    return text_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0FbukwgGO5x9"
   },
   "source": [
    "### Definir evaluación\n",
    "\n",
    "Esta función imprime la matriz de confusión, el reporte de clasificación y las metricas usadas en la competencia:\n",
    "\n",
    "\n",
    "- `auc`\n",
    "- `kappa`\n",
    "- `accuracy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:24.009626Z",
     "start_time": "2019-08-21T19:53:24.002646Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "ZMqBB-Q2O5x_"
   },
   "outputs": [],
   "source": [
    "def evaulate(predicted, y_test, labels):\n",
    "    # Importante: al transformar los arreglos de probabilidad a clases,\n",
    "    # entregar el arreglo de clases aprendido por el clasificador. \n",
    "    # (que comunmente, es distinto a ['low', 'medium', 'high'])\n",
    "    predicted_labels = [labels[np.argmax(item)] for item in predicted]\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    print('Confusion Matrix for {}:\\n'.format(key))\n",
    "\n",
    "    # Classification Report\n",
    "    print(\n",
    "        confusion_matrix(y_test,\n",
    "                         predicted_labels,\n",
    "                         labels=['low', 'medium', 'high']))\n",
    "\n",
    "    print('\\nClassification Report')\n",
    "    print(\n",
    "        classification_report(y_test,\n",
    "                              predicted_labels,\n",
    "                              labels=['low', 'medium', 'high']))\n",
    "\n",
    "    # AUC\n",
    "    print(\"auc: \", auc(y_test, predicted))\n",
    "\n",
    "    # Kappa\n",
    "    print(\"kappa:\", cohen_kappa_score(y_test, predicted_labels))\n",
    "\n",
    "    # Accuracy\n",
    "    print(\"accuracy:\", accuracy_score(y_test, predicted_labels), \"\\n\")\n",
    "\n",
    "    print('------------------------------------------------------\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cross validation \n",
    "\n",
    "generar cross validation para manejar estadisticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def statisticsCV(clf, dataset, n_cv=10, eval_metric='accuracy'):\n",
    "    metrics = cross_val_score(clf, dataset.tweet, dataset.sentiment_intensity, cv= n_cv, scoring=eval_metric)\n",
    "    mean = np.mean(metrics)\n",
    "    std = np.std(metrics)\n",
    "    return mean, std, eval_metric"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graficos de metricas vs parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_statistics_all(train_set, data_keys, n_cv=10, eval_metric='accuracy'):\n",
    "    # defino todos los valores que voy a graficar en el eje x\n",
    "    # para random forest será numero de estimadores -> n_estimators\n",
    "    n_est = np.arange(5,500,300)\n",
    "    \n",
    "    # guardo los accuracy promedio por cada cross validation y su desviación estandar\n",
    "    scores = {'anger': [], 'fear':[], 'joy':[], 'sadness':[]}\n",
    "    scores_std = {'anger': [], 'fear':[], 'joy':[], 'sadness':[]}\n",
    "    \n",
    "    # se mostrarán 4 graficos, 1 por cada sentimiento, la posición es relativa a una matriz de 2x2\n",
    "    # de este modo, los datos los necesito tener con esa estructura de 2x2\n",
    "    keys_iterable = np.array(data_keys).reshape(2,2)\n",
    "    colors = ['tab:blue','tab:orange','tab:green','tab:red']\n",
    "    colors_iterable = np.array(colors).reshape(2,2)\n",
    "    \n",
    "    # imprimo la metrica que estoy estudiando\n",
    "    print(\"metricas: \"+ eval_metric)\n",
    "    \n",
    "    # por cada sentimiento saco las estadisticas y los guardo en cada dict\n",
    "    for sentiment in data_keys:\n",
    "        for n in n_est:\n",
    "            rfc = get_classifier(n)\n",
    "            if eval_metric == 'accuracy':\n",
    "                mean, std, eval_metric = statisticsCV(rfc, train_set[sentiment], n_cv, 'accuracy')\n",
    "            else:\n",
    "                mean, std, eval_metric = statisticsCV(rfc, train_set[sentiment], n_cv, make_scorer(cohen_kappa_score))\n",
    "            scores[sentiment].append(mean)\n",
    "            scores_std[sentiment].append(std)\n",
    "    \n",
    "    # defino los subplots como 2x2        \n",
    "    fig, axs = plt.subplots(2, 2, figsize=(15,15))\n",
    "    \n",
    "    # seteo cada grafico con sus respectivos datos\n",
    "    for row in [0,1]:\n",
    "        for col in [0,1]:\n",
    "            axs[row, col].plot(n_est, scores[keys_iterable[row][col]], colors_iterable[row][col])\n",
    "            axs[row, col].plot(n_est, np.array(scores[keys_iterable[row][col]]) + np.array(scores_std[keys_iterable[row][col]]), 'k--')\n",
    "            axs[row, col].plot(n_est, np.array(scores[keys_iterable[row][col]]) - np.array(scores_std[keys_iterable[row][col]]), 'k--')\n",
    "            axs[row, col].set_title(keys_iterable[row][col])\n",
    "    \n",
    "    # a cada grafico le agrego el nombre a sus ejes y su escala en el eje y\n",
    "    for ax in axs.flat:\n",
    "        ax.set(xlabel='cantidad de n_estimators', ylabel=eval_metric)\n",
    "        ax.set_ylim(0,1)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mostrar graficos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plot_statistics_all' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-d3f29b1c2da4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_statistics_all\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_keys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#plot_statistics_all(train, data_keys, eval_metric=make_scorer(cohen_kappa_score))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plot_statistics_all' is not defined"
     ]
    }
   ],
   "source": [
    "#accuracy\n",
    "plot_statistics_all(train, data_keys)\n",
    "\n",
    "#kappa\n",
    "plot_statistics_all(train, data_keys, eval_metric=make_scorer(cohen_kappa_score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bYQ5u7JQO5yG"
   },
   "source": [
    "### Ejecutar el clasificador para cierto dataset\n",
    "\n",
    "Clasifica un dataset. Retorna el modelo ya entrenado mas sus labels asociadas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:25.978116Z",
     "start_time": "2019-08-21T19:53:25.973129Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "0GI0S_jdO5yI",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def classify(dataset, key):\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_dataset(dataset)\n",
    "    text_clf = get_classifier()\n",
    "\n",
    "    # Entrenar el clasificador\n",
    "    #mark_negation(X_train, shallow=True)\n",
    "    text_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del set de prueba.\n",
    "    #mark_negation(X_test, shallow=True)\n",
    "    predicted = text_clf.predict_proba(X_test)\n",
    "    # Obtener las clases aprendidas.\n",
    "    learned_labels = text_clf.classes_\n",
    "\n",
    "    # Evaluar\n",
    "    evaulate(predicted, y_test, learned_labels)\n",
    "    return text_clf, learned_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8d7ua7yXO5yR"
   },
   "source": [
    "### Ejecutar el clasificador por cada dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:53:27.106461Z",
     "start_time": "2019-08-21T19:53:26.933924Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8761,
     "status": "ok",
     "timestamp": 1569271022471,
     "user": {
      "displayName": "F.I. S.B.",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AAuE7mDkeCvx5V5R3vmarSYx3LSktvwQxqeHN1TprOEk2Q=s64",
      "userId": "16803044934265976430"
     },
     "user_tz": 180
    },
    "id": "WOwdXPKvO5yT",
    "outputId": "d79e86da-503f-408f-bf60-a4986ea82a96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix for anger:\n",
      "\n",
      "[[  5  48   0]\n",
      " [  9 193   7]\n",
      " [  0  34  15]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.36      0.09      0.15        53\n",
      "      medium       0.70      0.92      0.80       209\n",
      "        high       0.68      0.31      0.42        49\n",
      "\n",
      "    accuracy                           0.68       311\n",
      "   macro avg       0.58      0.44      0.46       311\n",
      "weighted avg       0.64      0.68      0.63       311\n",
      "\n",
      "auc:  0.5074969041640859\n",
      "kappa: 0.1856463421151071\n",
      "accuracy: 0.684887459807074 \n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix for fear:\n",
      "\n",
      "[[ 13  80   2]\n",
      " [  7 198  22]\n",
      " [  0  67  26]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.65      0.14      0.23        95\n",
      "      medium       0.57      0.87      0.69       227\n",
      "        high       0.52      0.28      0.36        93\n",
      "\n",
      "    accuracy                           0.57       415\n",
      "   macro avg       0.58      0.43      0.43       415\n",
      "weighted avg       0.58      0.57      0.51       415\n",
      "\n",
      "auc:  0.39860893935859726\n",
      "kappa: 0.15441849816849818\n",
      "accuracy: 0.5710843373493976 \n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix for joy:\n",
      "\n",
      "[[ 15  54   1]\n",
      " [  7 159   8]\n",
      " [  0  35  19]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.68      0.21      0.33        70\n",
      "      medium       0.64      0.91      0.75       174\n",
      "        high       0.68      0.35      0.46        54\n",
      "\n",
      "    accuracy                           0.65       298\n",
      "   macro avg       0.67      0.49      0.51       298\n",
      "weighted avg       0.66      0.65      0.60       298\n",
      "\n",
      "auc:  0.39735609404403893\n",
      "kappa: 0.26549295774647896\n",
      "accuracy: 0.6476510067114094 \n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "\n",
      "Confusion Matrix for sadness:\n",
      "\n",
      "[[  5  73   0]\n",
      " [  4 135   6]\n",
      " [  0  49  12]]\n",
      "\n",
      "Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         low       0.56      0.06      0.11        78\n",
      "      medium       0.53      0.93      0.67       145\n",
      "        high       0.67      0.20      0.30        61\n",
      "\n",
      "    accuracy                           0.54       284\n",
      "   macro avg       0.58      0.40      0.36       284\n",
      "weighted avg       0.56      0.54      0.44       284\n",
      "\n",
      "auc:  0.3888647478361679\n",
      "kappa: 0.09865115048928852\n",
      "accuracy: 0.5352112676056338 \n",
      "\n",
      "------------------------------------------------------\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "classifiers = []\n",
    "learned_labels_array = []\n",
    "\n",
    "# Por cada llave en train ('anger', 'fear', 'joy', 'sadness')\n",
    "for key in train:\n",
    "    classifier, learned_labels = classify(train[key], key)\n",
    "    classifiers.append(classifier)\n",
    "    learned_labels_array.append(learned_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:37:43.169737Z",
     "start_time": "2019-08-21T19:37:43.166744Z"
    },
    "colab_type": "text",
    "id": "CsNJGodwO5ya"
   },
   "source": [
    "## Predecir target set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T19:50:59.474909Z",
     "start_time": "2019-08-21T19:50:59.469921Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "GmqXxh55O5yc"
   },
   "outputs": [],
   "source": [
    "def predict_target(dataset, classifier, labels):\n",
    "    # Predecir las probabilidades de intensidad de cada elemento del target set.\n",
    "    predicted = pd.DataFrame(classifier.predict_proba(dataset.tweet), columns=labels)\n",
    "    # Agregar ids\n",
    "    predicted['id'] = dataset.id.values\n",
    "    # Reordenar\n",
    "    predicted = predicted[['id', 'low', 'medium', 'high']]\n",
    "    return predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MLvivpH0O5yi"
   },
   "source": [
    "### Ejecutar la predicción y guardar archivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-21T20:00:10.724762Z",
     "start_time": "2019-08-21T20:00:10.576665Z"
    },
    "colab": {},
    "colab_type": "code",
    "id": "536ngErBO5yk",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predicted_target = {}\n",
    "\n",
    "if (not os.path.isdir('./predictions')):\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "else:\n",
    "    # Eliminar predicciones anteriores:\n",
    "    shutil.rmtree('./predictions')\n",
    "    os.mkdir('./predictions')\n",
    "\n",
    "for idx, key in enumerate(target):\n",
    "    # Predecir el target set\n",
    "    predicted_target[key] = predict_target(target[key], classifiers[idx],\n",
    "                                           learned_labels_array[idx])\n",
    "    # Guardar predicciones\n",
    "    predicted_target[key].to_csv('./predictions/{}-pred.txt'.format(key),\n",
    "                                 sep='\\t',\n",
    "                                 header=False,\n",
    "                                 index=False)\n",
    "\n",
    "# Crear archivo zip\n",
    "a = shutil.make_archive('predictions', 'zip', './predictions')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "UwaBrBQwO5xM"
   ],
   "name": "Primera_Iteracion.ipynb",
   "provenance": [
    {
     "file_id": "https://github.com/SanquirinoB/NLP_Proyects2019/blob/master/Tarea_1.ipynb",
     "timestamp": 1569113556085
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
